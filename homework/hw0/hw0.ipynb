{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9916c463-8a0b-4173-9c81-8b0538202582",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MCIS6273 Data Mining (Prof. Maull) / Fall 2021 / HW0 \n",
    "\n",
    "#### This assignment is worth up to 20 POINTS to your grade total if you complete it on time. \n",
    "\n",
    "Points Possible\n",
    "\n",
    "Due Date\n",
    "\n",
    "##### 20 Wednesday, Sep 1 @ Midnight \n",
    "\n",
    "Time Commitment (estimated) up to 20 hours\n",
    "\n",
    "-   GRADING: Grading will be aligned with the completeness of the\n",
    "    objectives.\n",
    "-   INDEPENDENT WORK: Copying, cheating, plagiarism and academic\n",
    "    dishonesty are not tolerated by University or course policy. Please\n",
    "    see the syllabus for the full departmental and University statement\n",
    "    on the academic code of honor.\n",
    "\n",
    "OBJECTIVES \n",
    "==========\n",
    "\n",
    "-   Familiarize yourself with the JupyterLab environment, Markdown and\n",
    "    Python\n",
    "-   Familiarize yourself with Github and basic git\n",
    "-   Explore JupyterHub Linux console integrating what you learned in the\n",
    "    prior parts of this homework\n",
    "-   Listen to the Talk Python[‘Podcast’] from June 25, 2021: A Path to\n",
    "    Data Science Interview with Sanyam Bhutani\n",
    "-   Explore Python for data munging and analysis, with an introduction\n",
    "    to CSV and Pandas\n",
    "\n",
    "WHAT TO TURN IN \n",
    "===============\n",
    "\n",
    "You are being encouraged to turn the assignment in using the provided\n",
    "Jupyter Notebook. To do so, make a directory in your Lab environment\n",
    "called homework/hw0. Put all of your ﬁles in that directory. Then zip\n",
    "that directory, rename it with your name as the ﬁrst part of the ﬁlename\n",
    "(e.g. maull\\_hw0\\_files.zip), then download it to your local machine,\n",
    "then upload the .zip to Blackboard.\n",
    "\n",
    "If you do not know how to do this, please ask, or visit one of the many\n",
    "tutorials out there on the basics of using zip in Linux.\n",
    "\n",
    "If you choose not to use the provided notebook, you will still need to\n",
    "turn in a .ipynb Jupyter Notebook and\n",
    "\n",
    "corresponding ﬁles according to the instructions in this homework.\n",
    "\n",
    "ASSIGNMENT TASKS \n",
    "================\n",
    "\n",
    "(0%) Familiarize yourself with the JupyterLab environment, Markdown and Python \n",
    "------------------------------------------------------------------------------\n",
    "\n",
    "As stated in the course announcement [Jupyter\n",
    "(https://jupyter.org)](https://jupyter.org) is the core platform we will\n",
    "be using in this course and is a popular platform for data scientists\n",
    "around the world. We have a JupyterLab setup for this course so that we\n",
    "can operate in a cloud-hosted environment, free from some of the\n",
    "resource constraints of running\n",
    "\n",
    "Jupyter on your local machine (though you are free to set it up on your\n",
    "own and seek my advice if you desire).\n",
    "\n",
    "You have been given the information about the Jupyter environment we\n",
    "have setup for our course, and the underlying Python environment will be\n",
    "using is the [Anaconda (https://anaconda.com)](https://anaconda.com)\n",
    "distribution. It is not necessary for this assignment, but you are free\n",
    "to look at the multitude of packages installed with Anaconda,\n",
    "\n",
    "though we will not use the majority of them explicitly. As you will soon\n",
    "ﬁnd out, Notebooks are an incredibly eﬀective way to mix code with\n",
    "narrative and you can create\n",
    "\n",
    "cells that are entirely code or entirely Markdown. Markdown (MD or md)\n",
    "is a highly readable text format that allows for easy documentation of\n",
    "text ﬁles, while allowing for HTML-based rendering of the text in a way\n",
    "that is style-independent.\n",
    "\n",
    "We will be using Markdown frequently in this course, and you will learn\n",
    "that there are many diﬀerent “ﬂavors” or Markdown. We will only be using\n",
    "the basic ﬂavor, but you will beneﬁt from exploring the “Github ﬂavored”\n",
    "\n",
    "Markdown, though you will not be responsible for using it in this course\n",
    "– only the “basic” ﬂavor. Please refer to the original course\n",
    "announcement about Markdown.\n",
    "\n",
    "§ THERE IS NOTHING TO TURN IN FOR THIS PART. Play with and become\n",
    "familiar with the basic functions of the Lab environment given to you\n",
    "online in the course Blackboard.\n",
    "\n",
    "§ PLEASE CREATE A MARKDOWN DOCUMENT CALLED semester\\_goals.md WITH 3 SEN\n",
    "TENCES/FRAGMENTS THAT ANSWER THE FOLLOWING QUESTION:\n",
    "\n",
    "• What do you wish to accomplish this semester in Data Mining? \n",
    "--------------------------------------------------------------\n",
    "\n",
    "Read the documentation for basic Markdown\n",
    "[here.](https://www.markdownguide.org/basic-syntax) Turn in the text .md\n",
    "ﬁle not the processed .html. In whatever you turn in, you must show the\n",
    "use of ALL the following:\n",
    "\n",
    "-   headings (one level is ﬁne),\n",
    "-   bullets,\n",
    "-   bold and italics\n",
    "\n",
    "Again, the content of your document needs to address the question above\n",
    "and it should live in the top level directory of your assignment\n",
    "submission. This part will be graded but no points are awarded for your\n",
    "answer.\n",
    "\n",
    "(0%) Familiarize yourself with Github and basic git \n",
    "---------------------------------------------------\n",
    "\n",
    "[Github (https://github.com)](https://github.com) is the de facto\n",
    "platform for open source software in the world based on the very popular\n",
    "[git (https://git-scm.org)](https://git-scm.org) version control system.\n",
    "Git has a sophisticated set of tools for version control based on the\n",
    "concept of local repositories for fast commits and remote repositories\n",
    "only when collaboration and remote synchronization is necessary. Github\n",
    "enhances git by providing tools and online hosting of public and private\n",
    "repositories to encourage and promote sharing and collaboration. Github\n",
    "hosts some of the world’s most widely used open source software.\n",
    "\n",
    "If you are already familiar with git and Github, then this part will be\n",
    "very easy! § CREATE A PUBLIC GITHUB REPO NAMED \"mcis6273-F21-datamining\"\n",
    "AND PLACE A\n",
    "\n",
    "README.MD FILE IN IT. Create your ﬁrst ﬁle called README.md at the top\n",
    "level of the repository. You can put whatever text you like in the ﬁle\n",
    "(If you like, use something like [lorem ipsum](https://lipsum.com/) to\n",
    "generate random sentences to place in the ﬁle.). Please include the link\n",
    "to your Github repository that now includes the minimal README.md.\n",
    "\n",
    "You don’t have to have anything elaborate in that ﬁle or the repo.\n",
    "\n",
    "(0%) Explore JupyterHub Linux console integrating what you learned in the prior parts of this \n",
    "=============================================================================================\n",
    "\n",
    "homework \n",
    "--------\n",
    "\n",
    "The Linux console in JupyterLab is a great way to perform command-line\n",
    "tasks and is an essential tool for basic scripting that is part of a\n",
    "data scientist’s toolkit. Open a console in the lab environment and\n",
    "familiarize yourself with your ﬁles and basic commands using git as\n",
    "indicated below.\n",
    "\n",
    "1.  In a new JupyterLab command line console, run the git clone command\n",
    "    to clone the new repository you created in the prior part. You will\n",
    "    want to read the documentation on this command (try here\n",
    "    https://www.git-scm.com/docs/git-clone to get a good start).\n",
    "2.  Within the same console, modify your README.md ﬁle, check it in and\n",
    "    push it back to your repository, using git push. Read the\n",
    "    documentation about git push.\n",
    "3.  The commands wget and curl are useful for grabbing data and ﬁles\n",
    "    from remote resources oﬀ the web. Read the documentation on each of\n",
    "    these commands by typing man wget or man curl in the terminal. Make\n",
    "    sure you pipe the output to a ﬁle or use the proper ﬂags to do so.\n",
    "\n",
    "§ THERE IS NOTHING TO TURN IN FOR THIS PART. \n",
    "============================================\n",
    "\n",
    "(30%) Listen to the Talk Python[‘Podcast’] from June 25, 2021: A Path to Data Science Interview with Sanyam Bhutani \n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Data science is one of the most important and “hot” disciplines today\n",
    "and there is a lot going on from data engineering to modeling and\n",
    "analysis. Bhutani is one of the top Kaggle leaders and in this interview\n",
    "shares his experience from computer science to data science, documenting\n",
    "some of the lessons he learned along the way.\n",
    "\n",
    "Please listen to this one hour podcast and answer some of the questions\n",
    "below. You can listen to it from one of\n",
    "\n",
    "the two links below:\n",
    "\n",
    "- Talk Python[‘Podcast’] landing page\n",
    "-   direct link to mp3 ﬁle\n",
    "\n",
    "§ PLEASE ANSWER THE FOLLOWING QUESTIONS AFTER LISTENING TO THE PODCAST: \n",
    "=======================================================================\n",
    "\n",
    "1.  List 3 things that you learned from this podcast?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "- Get practical with Tutorials\n",
    "\n",
    "    Get Practical with Tutorials\n",
    "    Follow along with Tutorials and experiment along the way.\n",
    "\n",
    "- Kaggle\n",
    "    \n",
    "    Kaggle is a great platform to find Data Science Projects and participate and learn along with the peers. Along with the increase in Learning curve, it also gets you excited pushing through the ranks just like a game.\n",
    "\n",
    "- Fast AI\n",
    "    \n",
    "    Fast AI is a great framework to start with, later a data scientist can easily migrate to other frameworks.\n",
    "\n",
    "2.  What is your reaction to the podcast? Pick at least one point Sanyam\n",
    "    brought up in the interview that you agree with and list your reason\n",
    "    why.\n",
    " **Answer**\n",
    " \n",
    "- As Sanyam mentioned, being active in a platform like Kaggle would significantly improve the learning curve in Data Science. \n",
    "    \n",
    "- I agree with him, especially in a career like Data Science, you find it hard to learn with Real Time examples, because unlike any other technology, acquiring practical knowledge in data science requires loads of data. \n",
    "    \n",
    "- With Kaggle, one can actually go through different competitions and huge repo of community published data & code to learn.\n",
    "    \n",
    "3.  After listening to the podcast, do you think you are more interested\n",
    "    or less interested in a career in Data Science?\n",
    " **Answer**\n",
    " \n",
    "- My interest towards Data Science is unaffected. I’m still glad that I chose Data Science as my Career. However, the podcast was very interesting and has good content. It provided some useful tools on how to get started and paved a path for a career in Data Science.\n",
    "\n",
    "\n",
    "(70%) Explore Python for data munging and analysis, with an introduction to CSV and Pandas \n",
    "------------------------------------------------------------------------------------------\n",
    "\n",
    "Python’s strengths shine when tasked with data munging and analysis. As\n",
    "we will learn throughout the course, there are a number of excellent\n",
    "data sources for open data of all kinds now available for the public.\n",
    "These open data sources are heralding the new era of transparency from\n",
    "all levels from small municipal data to big government data, from\n",
    "transportation, to science, to education.\n",
    "\n",
    "To warm up to such datasets, we will be working with an interesting\n",
    "dataset from the US Fish and Wildlife\n",
    "\n",
    "Service (FWS). This is a water quality data set taken from a managed\n",
    "national refuge in Virginia called Back Bay\n",
    "\n",
    "National Wildlife Refuge, which was established in 1938. As a function\n",
    "of being managed by the FWS, water quality samples are taken regularly\n",
    "from the marshes within the refuge. You can (and should) learn a little\n",
    "more about Back Bay from this link, since it has an interesting history,\n",
    "features and wildlife.\n",
    "\n",
    "•\n",
    "[https://www.fws.gov/refuge/Back\\_Bay/about.html](https://www.fws.gov/refuge/Back_Bay/about.html)\n",
    "\n",
    "The data we will be looking at can be found as a direct download from\n",
    "data.gov, the US data repository where\n",
    "\n",
    "many datasets from a variety of sources can be found – mostly related to\n",
    "the multitude of US government agencies. The dataset is a small water\n",
    "quality dataset with several decades of water quality data from Back\n",
    "Bay. We will be warming up to this dataset with a basic investigation\n",
    "into the shape, content and context of the data contained\n",
    "\n",
    "therein.\n",
    "\n",
    "In this part of the assignment, we will make use of Python libraries to\n",
    "pull the data from the endpoint and use\n",
    "\n",
    "[Pandas](https://pandas.pydata.org) to plot the data. The raw CSV data\n",
    "is readily imported into Pandas from the following URL:\n",
    "\n",
    "• [FWS Water Quality Data\n",
    "12/20/2020](https://catalog.data.gov/dataset/water-quality-data/resource/f4d736fd-ade9-4e3f-b8e0-ae7fd98b2f87)\n",
    "\n",
    "Please take a look at the page, on it you will notice a link to the raw\n",
    "CSV ﬁle:\n",
    "\n",
    "•\n",
    "[https://ecos.fws.gov/ServCat/DownloadFile/173741?Reference=117348](https://ecos.fws.gov/ServCat/DownloadFile/173741?Reference=117348)\n",
    "\n",
    "We are going to explore this dataset to learn a bit more about the water\n",
    "quality characteristics of Bay Bay over the past couple decades or so.\n",
    "\n",
    "§ WRITE THE CODE IN YOUR NOTEBOOK TO LOAD AND RESHAPE THE COMPLETE \n",
    "==================================================================\n",
    "\n",
    "CSV WATER QUALITY DATASET: \n",
    "--------------------------\n",
    "\n",
    "You will need to perform the following steps:\n",
    "\n",
    "1.  use pandas.read\\_csv() method to load the dataset into a Pandas\n",
    "    DataFrame;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50d8397e-71b6-4aff-bf36-7e94776eb0f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csvData = pd.read_csv('https://ecos.fws.gov/ServCat/DownloadFile/173741?Reference=117348')\n",
    "df = pd.DataFrame(csvData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7e154-8737-4230-af1c-2b3eeb3574eb",
   "metadata": {},
   "source": [
    "2.  clean the data so that the range of years is restricted to the 20\n",
    "    year period from 1999 to 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdc0efc4-ad28-4a9c-9f01-1396cf113c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 1999\n",
    "end_year = 2018\n",
    "filterLogic = (df['Year'] >= start_year) & (df['Year'] <= end_year)\n",
    "filteredData = df.loc[filterLogic]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e8277f-e7fd-4407-9f33-6b90f30eb4bd",
   "metadata": {},
   "source": [
    "3.  store the entire dataset back into a new CSV ﬁle called\n",
    "    back\\_bay\\_1998-2018\\_clean.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5223fa23-568a-43e7-93c9-7db7c131e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredData.to_csv('back_bay_1998-2018_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c6df8-d35a-4836-849c-8ea97f8d5e94",
   "metadata": {},
   "source": [
    "HINTS: Here are some a code hints you might like to study and use to\n",
    "craft a solution:\n",
    "\n",
    "-   study pandas.DataFrame.query()] to learn how to ﬁlter and query year\n",
    "    ranges\n",
    "-   study pandas.DataFrame.groupby() to understand how to group data\n",
    "\n",
    "§ USE PANDAS TO LOAD THE CSV DATA TO A DATAFRAME AND ANSWER THE FOL \n",
    "===================================================================\n",
    "\n",
    "LOWING QUESTIONS: \n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857c90c0-c200-4b0f-be90-a7d40bdd0518",
   "metadata": {},
   "source": [
    "1.  How many and what are the names of the columns in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24692192-a779-4fbb-868d-ee5f427365d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Count: 17\n",
      "\n",
      "Names: \n",
      "Site_Id\n",
      "Unit_Id\n",
      "Read_Date\n",
      "Salinity (ppt)\n",
      "Dissolved Oxygen (mg/L)\n",
      "pH (standard units)\n",
      "Secchi Depth (m)\n",
      "Water Depth (m)\n",
      "Water Temp (?C)\n",
      "Air Temp-Celsius\n",
      "Air Temp (?F)\n",
      "Time (24:00)\n",
      "Field_Tech\n",
      "DateVerified\n",
      "WhoVerified\n",
      "AirTemp (C)\n",
      "Year\n"
     ]
    }
   ],
   "source": [
    "columns = df.columns\n",
    "print('Column Count: '+str(columns.size) + '\\n\\nNames: ')\n",
    "for col in columns:\n",
    "  print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3956af33-4f8d-44f7-ac8d-066d20bc29d6",
   "metadata": {},
   "source": [
    "2.  What is the mean Dissolved Oxygen (mg/L) over the entire dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3e37d13-df04-4ee7-8a81-bb4a0478a673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Dissolved Oxygen: \n",
      "6.646263157894744\n"
     ]
    }
   ],
   "source": [
    "print('\\nMean Dissolved Oxygen: \\n' + str(df['Dissolved Oxygen (mg/L)'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e2d72f-f8f5-4c08-9a2e-e2934ffe6478",
   "metadata": {},
   "source": [
    "3.  Which year were the highest number of AirTemp (C) data points\n",
    "    collected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f339f73-8fef-4f94-bb85-7d0a09946228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year with Highest number of Data Points:\n",
      "\n",
      "2009\n"
     ]
    }
   ],
   "source": [
    "dropped_values = df[df['AirTemp (C)'].notnull()]\n",
    "temp_data_points = dropped_values.groupby(['Year']).size().reset_index(name='counts').sort_values(['counts'],ascending=False)\n",
    "print('\\nYear with Highest number of Data Points:\\n')\n",
    "print(temp_data_points['Year'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2d7d4c-f04c-49ff-ae39-8649bbb35120",
   "metadata": {},
   "source": [
    "4.  Which year were the least number of AirTemp (C) data points\n",
    "    collected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f08d068-5c15-4766-8112-66e384135484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year with Lowest number of Data Points:\n",
      "\n",
      "1899\n"
     ]
    }
   ],
   "source": [
    "print('\\nYear with Lowest number of Data Points:\\n')\n",
    "print(temp_data_points['Year'].iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef0de7-63a0-45df-afa6-7c5d3ffec750",
   "metadata": {},
   "source": [
    "To answer these questions, you’ll need to dive further into Pandas,\n",
    "which is the standard tool in the Python data science stack for loading,\n",
    "manipulating, transforming, analyzing and preparing data as input to\n",
    "other tools such as [Numpy\n",
    "(http://www.numpy.org/),](http://www.numpy.org/) [SciKitLearn\n",
    "(http://scikit-learn.org/stable/index.html),](http://scikit-learn.org/stable/index.html)\n",
    "[NLTK (http://www.nltk.org/)](http://www.nltk.org/) and others.\n",
    "\n",
    "For this assignment, you will only need to learn how to load and select\n",
    "data using Pandas.\n",
    "\n",
    "-   LOADING DATA The core data structure in Pandas is the DataFrame. You\n",
    "    will need to visit the Pandas documentation\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/reference/) to learn\n",
    "    more about the library, but to help you along with a hint, read the\n",
    "    documentation on the pandas.read\\_csv() method.\n",
    "-   SELECTING DATA The tutorial here on indexing and selecting should be\n",
    "    of great use in understanding how to index and select subsets of the\n",
    "    data to answer the questions.\n",
    "-   GROUPING DATA You may use DataFrame.value\\_counts() or\n",
    "    DataFrame.groupby() to group the data you need for these questons.\n",
    "    You will also ﬁnd\n",
    "    DataFrame.groupby()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html?highlight=groupby\\#pandas.DataFrame.groupby)\n",
    "    and [DataFrame.describe()‘ very useful.\n",
    "\n",
    "CODE HINTS \n",
    "----------\n",
    "\n",
    "Here is example code that should give you clues about the structure of\n",
    "your code for this part.\n",
    "\n",
    "### import pandas as pd \n",
    "\n",
    "df = pd.read\\_csv('your\\_json\\_file.csv')\n",
    "\n",
    "\\# code for question 1 ... and so on\n",
    "\n",
    "§ EXPLORING WATER SALINITY IN THE DATA \n",
    "--------------------------------------\n",
    "\n",
    "The Back Bay refuge is on the eastern coast of Virginia and to the east\n",
    "is the Atlantic Ocean. Salinity is a measure of the salt concentration\n",
    "of water, and you can learn a little more about salinity in water\n",
    "[here.](https://www.usgs.gov/special-topic/water-science-school/science/saline-water-and-salinity?qt-science_center_objects=0#qt-science_center_objects)\n",
    "\n",
    "You will notice that there is a Site\\_Id variable in the data, which we\n",
    "will ﬁnd refers to the ﬁve sampling locations (see the [documentation\n",
    "here)](https://ecos.fws.gov/ServCat/Reference/Profile/117348) of (1) the\n",
    "Bay, (2) D-Pool (ﬁshing pond), (3) C-Pool, (4) B-Pool and (5) A-Pool.\n",
    "\n",
    "The ppt in Salinity is the percent salinity, and so 1 ppt is equivalent\n",
    "to 10000 ppm salinity. Use this information to answer the following\n",
    "questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b28d64-f60a-4b56-a10c-fe90e74acbee",
   "metadata": {},
   "source": [
    "1.  Which sampling location has the highest mean ppt? What is the\n",
    "    equivalent ppm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c8d88bc-5475-45c5-8ca4-4d3d16a13497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Site ID with Highest Mean PPT:\n",
      "\n",
      "Bay\n",
      "\n",
      "Equivalent PPM of Highest Mean PPT:\n",
      "\n",
      "14831.53638814016\n",
      "\n",
      "Equivalent PPM of Highest PPT:\n",
      "\n",
      "11005000.0\n"
     ]
    }
   ],
   "source": [
    "mean_ppt = df.groupby('Site_Id')['Salinity (ppt)'].mean().reset_index(name='mean').sort_values(['mean'],ascending=False)\n",
    "high_ppt = df.groupby('Site_Id')['Salinity (ppt)'].sum().reset_index(name='sum').sort_values(['sum'],ascending=False)\n",
    "print('\\nSite ID with Highest Mean PPT:\\n')\n",
    "print(mean_ppt['Site_Id'].iloc[0])\n",
    "print('\\nEquivalent PPM of Highest Mean PPT:\\n')\n",
    "print(mean_ppt['mean'].iloc[0] * 10000)\n",
    "print('\\nEquivalent PPM of Highest PPT:\\n')\n",
    "print(high_ppt['sum'].iloc[0] * 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4410fd8-d47d-4b7f-86cf-821e1b01b793",
   "metadata": {},
   "source": [
    "2.  When looking at the mean ppt, which location would you infer is\n",
    "    furthest from the inﬂuence of ocean water inﬂows? (Assume that\n",
    "    higher salinity correlates to closer proximity to the ocean.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ed6512b-6090-4669-82a3-507fcf4b09d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Furthest Location from Ocean Water:\n",
      "\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "print('\\nFurthest Location from Ocean Water:\\n')\n",
    "print(mean_ppt['Site_Id'].iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef390bd-ac4d-48d7-b546-4677c4a29026",
   "metadata": {},
   "source": [
    "3.  Dig a little deeper into #2, and write why there may be some\n",
    "    uncertainty in your answer? (hint: certainty is improved by\n",
    "    consistency in data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee7d79-a814-435f-8768-be1d2716d527",
   "metadata": {},
   "source": [
    "Distance cannot be determined between the group of locations with Salinity as 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d059cf53-68ea-4783-9ef4-7c389f60835f",
   "metadata": {},
   "source": [
    "4.  Use the data to determine the correlation between Salinity (ppt) and\n",
    "    pH (standard units). Use the DataFrame.corr(). You just need to\n",
    "    report the correlation value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94df0115-bf21-4410-a5dd-a59bbf014859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation:\n",
      "\n",
      "0.29607528012371914\n",
      "\n",
      "\n",
      "Correlation matrix using Kendall method:\n",
      "\n",
      "                     Salinity (ppt)  pH (standard units)\n",
      "Salinity (ppt)             1.000000             0.059888\n",
      "pH (standard units)        0.059888             1.000000\n"
     ]
    }
   ],
   "source": [
    "print('Correlation:\\n')\n",
    "print(df['Salinity (ppt)'].corr(df['pH (standard units)']))\n",
    "print('\\n\\nCorrelation matrix using Kendall method:\\n')\n",
    "print(df[['Salinity (ppt)','pH (standard units)']].corr(method='kendall'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2682539-25bd-49f3-be7f-16a6797b2a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
